
 â–â–›â–ˆâ–ˆâ–ˆâ–œâ–Œ   Claude Code v2.0.10
â–â–œâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–›â–˜  Sonnet 4.5 Â· Claude Max
  â–˜â–˜ â–â–    /Users/calebsandler/Code Repos/SWAB/mentor-database

> /usage 
  âŽ¿ Â Status dialog dismissed

> Okay so I have been working on this contact database a TON and we are almost there. I forst built the backend sync functionality and verified that,
 then begin to build the front end that is configurable,etc. I have gotten to the point where I have a sync that you can setup in settings and 
manually rn in the we app. The only issue is that the sync process only fetches the raw  data and populates those tables. It does not populate the 
mentor table, the mn tasks, mn errors, etc. I want the baseline sync to grab all of the raw data, run the etl, and then sync the raw with our current
 state. I do not want the givebutter archiving function to be a part of this baseline and I do not want the contact csv import to be a part of this 
sync. The goal here is just to say: weve looked in all the places mentor and this is what we have for you 

âº I'll help you extend the sync process to include the ETL and populate the mentor-related tables. Let me first explore the current sync
  implementation to understand what's missing.

âº Read(backend/core/sync/givebutter-contacts.ts)
  âŽ¿ Â Read 184 lines (ctrl+o to expand)

âº Read(backend/core/sync/givebutter-members.ts)
  âŽ¿ Â Read 168 lines (ctrl+o to expand)

âº Search(pattern: "**/etl*.ts")
  âŽ¿ Â Found 0 files 

âº Search(pattern: "**/sync*.ts")
  âŽ¿ Â Found 8 files (ctrl+o to expand)

âº Search(pattern: "backend/**/*.ts")
  âŽ¿ Â Found 17 files (ctrl+o to expand)

âº Search(pattern: "mentor table|mn_tasks|mn_errors", type: "ts")
  âŽ¿ Â Found 3 files (ctrl+o to expand)

âº Search(pattern: "src/**/*sync*.{ts,tsx}")
  âŽ¿ Â Found 0 files 

âº Read(backend/core/sync/all.ts)
  âŽ¿ Â Read 67 lines (ctrl+o to expand)

âº Read(backend/core/etl/process.ts)
  âŽ¿ Â Read 741 lines (ctrl+o to expand)

âº Search(pattern: "src/app/**/*sync*.{ts,tsx}")
  âŽ¿ Â Found 0 files 

âº Search(pattern: "sync.*api|api.*sync", glob: "src/**/*.{ts,tsx}", type: "ts")
  âŽ¿ Â Found 8 files (ctrl+o to expand)

âº Search(pattern: "src/app/api/**/*.ts")
  âŽ¿ Â Found 6 files (ctrl+o to expand)

âº Read(src/app/api/sync/run/route.ts)
  âŽ¿ Â Read 287 lines (ctrl+o to expand)

âº Read(backend/core/sync/jotform-signups.ts)
  âŽ¿ Â Read 177 lines (ctrl+o to expand)

âº Read(backend/core/sync/jotform-setup.ts)
  âŽ¿ Â Read 166 lines (ctrl+o to expand)

âº I can see the issue. The web app sync only fetches raw data from APIs but doesn't run the ETL process. I'll modify the sync route to:

  1. Remove the Givebutter Contacts CSV sync (as you requested)
  2. Add the ETL process as the final step to populate mentors, mn_tasks, and mn_errors

âº Update(src/app/api/sync/run/route.ts)
  âŽ¿ Â Updated src/app/api/sync/run/route.ts with 2 additions and 2 removals
       22        script: 'backend/core/sync/givebutter-members.ts',
       23      },
       24      {
       25 -      name: 'Givebutter Contacts',
       26 -      script: 'backend/core/sync/givebutter-contacts.ts',
       25 +      name: 'ETL Process',
       26 +      script: 'backend/core/etl/process.ts',
       27      },
       28    ];
       29    

âº Now I need to update the output parser to handle ETL process statistics:

âº Update(src/app/api/sync/run/route.ts)
  âŽ¿ Â Updated src/app/api/sync/run/route.ts with 15 additions and 2 removals              
       276        stats.recordsInserted = parseInt(syncedMatch[1], 10);
       277      }
       278    
       279 -    // Look for patterns like "Errors: 3"
       280 -    const errorsMatch = output.match(/Errors:\s*(\d+)/i);
       279 +    // ETL-specific patterns
       280 +    // Look for "Unique mentors: X"
       281 +    const mentorsMatch = output.match(/Unique mentors:\s*(\d+)/i);
       282 +    if (mentorsMatch) {
       283 +      stats.recordsProcessed = parseInt(mentorsMatch[1], 10);
       284 +    }
       285 +  
       286 +    // Look for "Mentors: X" in the Inserted section
       287 +    const insertedMatch = output.match(/Inserted:[\s\S]*?Mentors:\s*(\d+)/i);
       288 +    if (insertedMatch) {
       289 +      stats.recordsInserted = parseInt(insertedMatch[1], 10);
       290 +    }
       291 +  
       292 +    // Look for patterns like "Errors: 3" or "Errors logged: 3"
       293 +    const errorsMatch = output.match(/Errors(?: logged)?:\s*(\d+)/i);
       294      if (errorsMatch) {
       295        stats.recordsFailed = parseInt(errorsMatch[1], 10);
       296      }

âº Done! I've updated the sync process to:

  Changes made:
  1. Removed Givebutter Contacts CSV sync (as requested)
  2. Added ETL Process as the final step

  New sync flow:
  1. Jotform Signups â†’ mn_signups_raw
  2. Jotform Setup â†’ funds_setup_raw
  3. Givebutter Members â†’ campaign_members_raw
  4. ETL Process â†’ mentors, mn_tasks, mn_errors

  Now when you run sync from the web app, it will fetch all the raw data from your sources and automatically run the ETL to populate the mentor
  tables with the transformed, matched, and validated data.

  The sync log will also properly track ETL statistics like number of mentors processed and errors logged.

> Ok good. The idea here is the other function slike archiving duplicate contacts, setting the text field based on the current text being sent out, 
things like that will be associated with "features" but the main pipeline will remian the same and periodically run & upsert correct? 

âº Exactly! You've got the right architecture:

  Main Pipeline (baseline sync):
  - Periodically runs (manual or scheduled)
  - Fetches fresh data from sources â†’ raw tables
  - Runs ETL â†’ upserts to mentors, mn_tasks, mn_errors, mn_gb_import
  - Always gives you current state: "here's what we have from all mentor sources"
  - Non-destructive: uses upserts so existing data is updated, not lost

  Separate Features (on-demand operations):
  - Archive duplicate contacts - manual Givebutter cleanup operation
  - Update text message templates - campaign-specific messaging updates
  - Export for SMS campaigns - generate contact lists with custom filters
  - Manual contact imports - one-off CSV uploads
  - Givebutter export/sync - push mn_gb_import to Givebutter

  These features will be separate endpoints/functions that operate on the already-synced data in the main tables, letting you build workflows on top
  of the clean baseline without polluting the core sync pipeline.

  This keeps the pipeline simple, reliable, and always gives you a "source of truth" snapshot.

> Ok lets test it then! Claer the database and we will start from scratch and test. This feels super promising 

âº Let's clear everything and test from scratch! I'll create a quick script to wipe all the tables.

âº Write(backend/core/utilities/clear-database.ts)
  âŽ¿ Â User rejected write to backend/core/utilities/clear-database.ts
        1 +  /**
        2 +   * Clear all database tables (for testing)
        3 +   *
        4 +   * WARNING: This will delete ALL data from raw and main tables
        5 +   * Usage: npm run clear-db
        6 +   */
        7 +  
        8 +  import dotenv from 'dotenv';
        9 +  import { resolve } from 'path';
       10 +  import { createClient } from '@supabase/supabase-js';
       11 +  import { getSupabaseConfig } from '../config/supabase';
       12 +  
       13 +  dotenv.config({ path: resolve(process.cwd(), '.env.local') });
       14 +  
       15 +  async function clearDatabase() {
       16 +    console.log('\n' + '='.repeat(80));
       17 +    console.log('ðŸ—‘ï¸  CLEARING ALL DATABASE TABLES');
       18 +    console.log('='.repeat(80) + '\n');
       19 +  
       20 +    const config = getSupabaseConfig();
       21 +    const supabase = createClient(config.url, config.serviceRoleKey || config.anonKey);
       22 +  
       23 +    console.log(`ðŸ”— Connected to: ${config.url}\n`);
       24 +  
       25 +    try {
       26 +      console.log('âš ï¸  Clearing tables in order (respecting foreign keys)...\n');
       27 +  
       28 +      // Clear FK references first
       29 +      await supabase.from('campaign_members_raw').update({ mn_id: null }).not('mn_id', 'is', null);
       30 +      console.log('âœ… Cleared campaign_members_raw FK references');
       31 +  
       32 +      // Clear dependent tables (tasks and errors reference mentors)
       33 +      await supabase.from('mn_tasks').delete().gte('mn_id', '');
       34 +      console.log('âœ… Cleared mn_tasks');
       35 +  
       36 +      await supabase.from('mn_errors').delete().gte('created_at', '1970-01-01');
       37 +      console.log('âœ… Cleared mn_errors');
       38 +  
       39 +      await supabase.from('mn_gb_import').delete().gte('mn_id', '');
       40 +      console.log('âœ… Cleared mn_gb_import');
       41 +  
       42 +      // Clear main mentor table
       43 +      await supabase.from('mentors').delete().gte('mn_id', '');
       44 +      console.log('âœ… Cleared mentors');
       45 +  
       46 +      // Clear raw tables
       47 +      await supabase.from('mn_signups_raw').delete().gte('submission_id', '');
       48 +      console.log('âœ… Cleared mn_signups_raw');
       49 +  
       50 +      await supabase.from('funds_setup_raw').delete().gte('submission_id', '');
       51 +      console.log('âœ… Cleared funds_setup_raw');
       52 +  
       53 +      await supabase.from('campaign_members_raw').delete().gte('member_id', 0);
       54 +      console.log('âœ… Cleared campaign_members_raw');
       55 +  
       56 +      await supabase.from('full_gb_contacts').delete().gte('contact_id', 0);
       57 +      console.log('âœ… Cleared full_gb_contacts');
       58 +  
       59 +      console.log('\n' + '='.repeat(80));
       60 +      console.log('âœ… DATABASE CLEARED - Ready for fresh sync!');
       61 +      console.log('='.repeat(80) + '\n');
       62 +  
       63 +    } catch (error) {
       64 +      console.error('\nâŒ Error clearing database:', error);
       65 +      process.exit(1);
       66 +    }
       67 +  }
       68 +  
       69 +  clearDatabase();

> We are using a dockerized dev container. I literally mean kill it and restart 

âº Got it! Let's restart the Docker containers to get a fresh database:

âº Bash(docker-compose down -v)
  âŽ¿ Â Interrupted Â· What should Claude do instead?

> use the supabase cli please 

âº Bash(supabase db reset)
  âŽ¿ Â Resetting local database...   
     Recreating database...
     Initialising schema...
     â€¦ +12 lines (ctrl+o to expand)

âº Perfect! Database is reset and ready. Fresh start with all migrations applied.

  Now you can test the full sync pipeline through the web app - it should:
  1. Fetch Jotform signups â†’ mn_signups_raw
  2. Fetch Jotform setup â†’ funds_setup_raw
  3. Fetch Givebutter members â†’ campaign_members_raw
  4. Run ETL â†’ populate mentors, mn_tasks, mn_errors, mn_gb_import

  Go ahead and trigger the sync from the settings page!

> Ok for some reason there are a HOST of new issues that were not there before. THis was the console output "(base) calebsandler@Mac mentor-database 
% npm run build && npm run dev

> swab-mentor-database@1.0.0 build
> next build

   â–² Next.js 15.5.4
   - Environments: .env.local

   Creating an optimized production build ...
 âœ“ Compiled successfully in 1154ms
 âœ“ Linting and checking validity of types    
 âœ“ Collecting page data    
 âœ“ Generating static pages (12/12)
 âœ“ Collecting build traces    
 âœ“ Finalizing page optimization    

Route (app)                                 Size  First Load JS    
â”Œ â—‹ /                                      141 B         102 kB
â”œ â—‹ /_not-found                            999 B         103 kB
â”œ Æ’ /api/sync/config                       141 B         102 kB
â”œ Æ’ /api/sync/discover-givebutter          141 B         102 kB
â”œ Æ’ /api/sync/discover-jotform             141 B         102 kB
â”œ Æ’ /api/sync/run                          141 B         102 kB
â”œ Æ’ /api/sync/test-keys                    141 B         102 kB
â”œ Æ’ /api/sync/upload-csv                   141 B         102 kB
â”œ â—‹ /settings                            6.68 kB         118 kB
â”” â—‹ /sync                                 3.5 kB         118 kB
+ First Load JS shared by all             102 kB
  â”œ chunks/255-4efeec91c7871d79.js       45.8 kB
  â”œ chunks/4bd1b696-c023c6e3521b1417.js  54.2 kB
  â”” other shared chunks (total)          1.92 kB


â—‹  (Static)   prerendered as static content
Æ’  (Dynamic)  server-rendered on demand


> swab-mentor-database@1.0.0 dev
> next dev

 âš  Port 3000 is in use by process 92549, using available port 3005 instead.
   â–² Next.js 15.5.4
   - Local:        http://localhost:3005
   - Network:      http://192.168.1.175:3005
   - Environments: .env.local

 âœ“ Starting...
 âœ“ Ready in 1294ms
 â—‹ Compiling /settings ...
 âœ“ Compiled /settings in 1927ms (679 modules)
 GET /settings 200 in 2167ms
 âš  Cross origin request detected from 192.168.1.175 to /_next/* resource. In a future major version of Next.js, you will need to explicitly 
configure "allowedDevOrigins" in next.config to allow this.
Read more: https://nextjs.org/docs/app/api-reference/config/next-config-js/allowedDevOrigins
 GET /settings 200 in 39ms
 GET /settings 200 in 35ms
 âœ“ Compiled / in 165ms (675 modules)
 GET / 200 in 224ms
 GET / 200 in 11ms
 âœ“ Compiled /_not-found in 69ms (666 modules)
 GET /mentors 404 in 168ms
 GET / 200 in 13ms
 âœ“ Compiled /sync in 89ms (681 modules)
 GET /sync 200 in 171ms
 âœ“ Compiled /api/sync/config in 188ms (773 modules)
 GET /api/sync/config 200 in 309ms
 GET / 200 in 29ms
 GET /communications 404 in 11ms
 GET /communications 404 in 85ms
 GET /sync 200 in 12ms
 GET /api/sync/config 200 in 11ms
 GET /api/sync/config 200 in 8ms
 GET /settings?tab=api-config 200 in 16ms
 GET /api/sync/config 200 in 14ms
 GET /api/sync/config 200 in 9ms
 GET /api/sync/config 200 in 11ms
 GET /api/sync/config 200 in 8ms
 âœ“ Compiled /api/sync/test-keys in 107ms (775 modules)
 POST /api/sync/test-keys 200 in 975ms
 âœ“ Compiled /api/sync/discover-givebutter in 194ms (779 modules)
 POST /api/sync/discover-jotform 200 in 670ms
 POST /api/sync/discover-givebutter 200 in 1242ms
 POST /api/sync/test-keys 200 in 689ms
 POST /api/sync/discover-jotform 200 in 377ms
 POST /api/sync/discover-givebutter 200 in 1190ms
 âœ“ Compiled /api/sync/upload-csv in 134ms (700 modules)
 POST /api/sync/upload-csv 200 in 297ms
 âœ“ Compiled /api/sync/config in 68ms (511 modules)
 POST /api/sync/config 200 in 153ms
 GET /api/sync/config 200 in 12ms
 âœ“ Compiled /api/sync/run in 144ms (780 modules)
[dotenv@17.2.3] injecting env (0) from .env.local -- tip: âœ… audit secrets and track compliance: https://dotenvx.com/ops


================================================================================
ðŸ“¥ SYNCING JOTFORM SIGNUPS â†’ DATABASE
================================================================================


ðŸ”— Connected to Supabase: http://127.0.0.1:54321

ðŸ” Fetching submissions from form 250685983663169...

âœ… Found 568 submissions


ï¿½ï¿½ Processing submissions...


   Processed 50 submissions...

   Processed 100 submissions...

   Processed 150 submissions...

   Processed 200 submissions...

   Processed 250 submissions...

   Processed 300 submissions...

   Processed 350 submissions...

   Processed 400 submissions...

   Processed 450 submissions...

   Processed 500 submissions...

   Processed 550 submissions...


================================================================================
âœ… SYNC COMPLETE

================================================================================
ðŸ“Š Results:
   Total submissions: 568
   Synced successfully: 568
   Errors: 0


âœ… backend/core/sync/jotform-signups.ts completed successfully
[dotenv@17.2.3] injecting env (0) from .env.local -- tip: ðŸ” encrypt with Dotenvx: https://dotenvx.com


================================================================================
ðŸ“¥ SYNCING JOTFORM SETUP â†’ DATABASE
================================================================================


ðŸ”— Connected to Supabase: http://127.0.0.1:54321

ðŸ” Fetching submissions from form 250754977634066...

âœ… Found 434 submissions


ðŸ“ Processing submissions...


   Processed 50 submissions...

   Processed 100 submissions...

   Processed 150 submissions...

   Processed 200 submissions...

   Processed 250 submissions...

   Processed 300 submissions...

   Processed 350 submissions...

   Processed 400 submissions...


================================================================================
âœ… SYNC COMPLETE

================================================================================
ðŸ“Š Results:
   Total submissions: 434
   Synced successfully: 434
   Errors: 0


âœ… backend/core/sync/jotform-setup.ts completed successfully
[dotenv@17.2.3] injecting env (0) from .env.local -- tip: ðŸ“¡ add observability to secrets: https://dotenvx.com/ops


================================================================================

ðŸ“¥ SYNCING GIVEBUTTER MEMBERS â†’ DATABASE
================================================================================


ðŸ”— Connected to Supabase: http://127.0.0.1:54321

ðŸ” Looking up campaign: CQVG3W...

âœ… Found campaign: SWAB Event Day Fundraiser 2025 (ID: 376599)


ðŸ” Fetching campaign members (paginated, 20 per page)...


   Page 1: fetched 20 members (total so far: 20/490)

   Page 2: fetched 20 members (total so far: 40/490)

   Page 3: fetched 20 members (total so far: 60/490)

   Page 4: fetched 20 members (total so far: 80/490)

   Page 5: fetched 20 members (total so far: 100/490)

   Page 6: fetched 20 members (total so far: 120/490)

   Page 7: fetched 20 members (total so far: 140/490)

   Page 8: fetched 20 members (total so far: 160/490)

   Page 9: fetched 20 members (total so far: 180/490)

   Page 10: fetched 20 members (total so far: 200/490)

   Page 11: fetched 20 members (total so far: 220/490)

   Page 12: fetched 20 members (total so far: 240/490)

   Page 13: fetched 20 members (total so far: 260/490)

   Page 14: fetched 20 members (total so far: 280/490)

   Page 15: fetched 20 members (total so far: 300/490)

   Page 16: fetched 20 members (total so far: 320/490)

   Page 17: fetched 20 members (total so far: 340/490)

   Page 18: fetched 20 members (total so far: 360/490)

   Page 19: fetched 20 members (total so far: 380/490)

   Page 20: fetched 20 members (total so far: 400/490)

   Page 21: fetched 20 members (total so far: 420/490)

   Page 22: fetched 20 members (total so far: 440/490)

   Page 23: fetched 20 members (total so far: 460/490)

   Page 24: fetched 20 members (total so far: 480/490)

   Page 25: fetched 10 members (total so far: 490/490)


âœ… Total members fetched: 490

ðŸ“ Processing members...


   Processed 50 members...

   Processed 100 members...

   Processed 150 members...

   Processed 200 members...

   Processed 250 members...

   Processed 300 members...

   Processed 350 members...

   Processed 400 members...

   Processed 450 members...


================================================================================
âœ… SYNC COMPLETE

================================================================================
ðŸ“Š Results:
   Total members: 490
   Synced successfully: 490
   Errors: 0


âœ… backend/core/sync/givebutter-members.ts completed successfully
[dotenv@17.2.3] injecting env (0) from .env.local -- tip: ðŸ” prevent building .env in docker: https://dotenvx.com/prebuild


================================================================================

ðŸ”„ ETL PROCESS: RAW â†’ MAIN TABLES (FINAL RESTRUCTURE)
================================================================================

âš™ï¸  Loaded configuration:
   Custom Fields: 7 fields (Year: 2025)
   Tags: Mentors 2025 (Year: 2025)


ðŸ“¥ Loading raw data...


âœ… Loaded:

   Signups: 568
   Setup: 434
   Members: 490
   Contacts: 0

ðŸ” Validating mn_id presence...


âœ… Validated 568 signups
âš ï¸  Missing mn_id: 0 placeholders created


ðŸ” Deduplicating signups by phone...


âœ… 568 signups â†’ 555 unique by phone
   Duplicates removed: 13

âš™ï¸  Processing mentors...


âœ… Processed 555 mentors

ðŸ’¾ Upserting to main tables...


âœ… Inserted:
   Mentors: 555

   Tasks: 555

ðŸ” Detecting Givebutter duplicate contacts...

âœ…  Found 0 duplicate contact groups in Givebutter

ï¿½ï¿½ Populating mn_gb_import...


âœ… Populated mn_gb_import with 555 rows

ðŸ”— Linking campaign members to mentors...


âœ… 372 campaign members linked to mentors


âœ… No errors to log


================================================================================
âœ… ETL COMPLETE
================================================================================
ðŸ“Š Stats:
   Raw signups: 568
   Unique mentors: 555

   With Givebutter contact: 0

   Without contact: 555
   Errors logged: 0


ðŸ“ˆ Status Breakdown:

   Complete: 0
   Needs fundraising: 327
   Needs page: 21
   Needs setup: 165


âœ… backend/core/etl/process.ts completed successfully
 POST /api/sync/run 200 in 27250ms" and as you can see there is just so any different logical issues and just htings not matching up in general. 
Looking at the database, I am going to go table by table\
No issues in c_m_r\
There is no info in full_gb_contacts at all meaning there was an issue\
No issues with f_s_r\
Mentors\
- Every person is supposed to have a preffered name. If they specifically mention that they have a preffered name different than their legal name in 
1 of 2 places (jotform sign up or past givebutter contact from when they mentored and the info was wrong) then they will have a legal name that is 
different from their preffered. If they dont have a preffered name different than their legal, I want their legal to be coppied over to preffered so 
every mentor has at least a preffered name housed in prefx fields in many cases, a first name, an optional middle name, and a last name\
- There are no givebutter contact ids AT ALL which is a hige issue and is likely logical\
- The goal is for preffered name to be display name so we dont ahve to ahve so many\
There are no entries in the mn_errors table at all (maybe could be correct might not be?) There should at least be records of where duplicates were 
found in different places I believe. Like if there are theoritically "duplicates" from jotform, they should be noted. If there are multiple instance 
of contacts in givebutter with identical information to any of our mentors, they should be noted etc. I think there is logical issues here too\
The lack of givebutter contact ids for contacts that already exist continues and is mirrored to the mn_gb_import table as well\
In the mn_tasks table, there are no values for the campaign_joined_at field\
\
That concludes my comments about the output after integrating it into the web app. All of this is going to have to be fixed and likely, we are going 
to have to do a deep look into the logical paths of everything to make sure ot flows well and makes sense 

âº You're absolutely right - there are several logical issues. Let me address them systematically:

  Main Issues:
  1. full_gb_contacts is empty - We removed the CSV sync, so no Givebutter contacts exist
  2. Preferred name logic is broken - Should always populate, copying from first_name if no prefix
  3. No error logging for duplicates - Duplicates being removed silently
  4. No campaign_joined_at - Not tracked by Givebutter API
  5. Display name redundancy - Should just use preferred_name

  Let me fix the ETL logic:

âº Read(backend/core/etl/process.ts)
  âŽ¿ Â Read 100 lines (ctrl+o to expand)

âº Update(backend/core/etl/process.ts)
  âŽ¿ Â Updated backend/core/etl/process.ts with 26 additions and 2 removals                                                          
       318    
       319        if (!existing || (signup.submitted_at && existing.submitted_at && signup.submitted_at > existing.submitted_at)) {
       320          if (existing) {
       321 -          duplicates.push(`${existing.mn_id} (older) vs ${signup.mn_id} (newer, kept)`);
       321 +          const dupMsg = `${existing.mn_id} (older) vs ${signup.mn_id} (newer, kept)`;
       322 +          duplicates.push(dupMsg);
       323 +  
       324 +          // Log to errors
       325 +          errors.push({
       326 +            mn_id: existing.mn_id,
       327 +            phone: normPhone,
       328 +            error_type: 'duplicate_signup',
       329 +            error_message: `Duplicate signup detected: ${dupMsg}. Kept most recent submission.`,
       330 +            severity: 'warning',
       331 +            source_table: 'mn_signups_raw',
       332 +            raw_data: { kept: signup.submission_id, discarded: existing.submission_id },
       333 +          });
       334          }
       335          phoneToSignup.set(normPhone, signup);
       336        } else {
       337 -        duplicates.push(`${signup.mn_id} (older) vs ${existing.mn_id} (newer, kept)`);
       337 +        const dupMsg = `${signup.mn_id} (older) vs ${existing.mn_id} (newer, kept)`;
       338 +        duplicates.push(dupMsg);
       339 +  
       340 +        // Log to errors
       341 +        errors.push({
       342 +          mn_id: signup.mn_id,
       343 +          phone: normPhone,
       344 +          error_type: 'duplicate_signup',
       345 +          error_message: `Duplicate signup detected: ${dupMsg}. Kept most recent submission.`,
       346 +          severity: 'warning',
       347 +          source_table: 'mn_signups_raw',
       348 +          raw_data: { kept: existing.submission_id, discarded: signup.submission_id },
       349 +        });
       350        }
       351      });
       352    

âº Update(backend/core/etl/process.ts)
  âŽ¿ Â Updated backend/core/etl/process.ts with 8 additions and 3 removals                       
       380        }
       381    
       382        // Name logic
       383 -      const preferredName = signup.prefix?.trim() || null;
       384 -      const firstName = signup.first_name || (preferredName || 'Unknown');
       385 -      const displayName = preferredName || firstName;
       383 +      // Legal names from Jotform
       384 +      const firstName = signup.first_name || 'Unknown';
       385 +      const middleName = signup.middle_name || null;
       386 +      const lastName = signup.last_name || 'Unknown';
       387 +  
       388 +      // Preferred name: use prefix if they specified one, otherwise use first_name
       389 +      const preferredName = signup.prefix?.trim() || firstName;
       390 +  
       391        const fullName = buildFullName(signup);
       392    
       393        // Match to Givebutter contact (phone first, then email)

âº Update(backend/core/etl/process.ts)
  âŽ¿ Â Updated backend/core/etl/process.ts with 1 addition and 2 removals
       140      first_name: string;
       141      middle_name?: string;
       142      last_name: string;
       143 -    preferred_name?: string;
       144 -    display_name: string;
       143 +    preferred_name: string;
       144      full_name: string;
       145      personal_email?: string;
       146      uga_email?: string;

âº Read(backend/core/etl/process.ts)
  âŽ¿ Â Read 30 lines (ctrl+o to expand)
  âŽ¿ Â Found 4 new diagnostic issues in 1 file (ctrl-o to expand)

âº Update(backend/core/etl/process.ts)
  âŽ¿ Â Updated backend/core/etl/process.ts with 3 additions and 4 removals
       488          gb_member_id: gbMemberId,
       489    
       490          first_name: firstName,
       491 -        middle_name: signup.middle_name,
       492 -        last_name: signup.last_name || 'Unknown',
       493 -        preferred_name: preferredName || undefined,
       494 -        display_name: displayName,
       491 +        middle_name: middleName,
       492 +        last_name: lastName,
       493 +        preferred_name: preferredName,
       494          full_name: fullName,
       495    
       496          personal_email: signup.personal_email,
  âŽ¿ Â Found 1 new diagnostic issue in 1 file (ctrl-o to expand)

âº Update(backend/core/etl/process.ts)
  âŽ¿ Â Updated backend/core/etl/process.ts with 1 addition and 1 removal                         
       381        // Name logic
       382        // Legal names from Jotform
       383        const firstName = signup.first_name || 'Unknown';
       384 -      const middleName = signup.middle_name || null;
       384 +      const middleName = signup.middle_name || undefined;
       385        const lastName = signup.last_name || 'Unknown';
       386    
       387        // Preferred name: use prefix if they specified one, otherwise use first_name

âº Read(supabase/migrations/00003_final_restructure.sql)
  âŽ¿ Â Read 50 lines (ctrl+o to expand)

âº Read(backend/core/etl/process.ts)
  âŽ¿ Â Read 30 lines (ctrl+o to expand)

âº Read(backend/core/etl/process.ts)
  âŽ¿ Â Read 25 lines (ctrl+o to expand)

âº Update(backend/core/etl/process.ts)
  âŽ¿ Â Updated backend/core/etl/process.ts with 1 addition and 1 removal                
       674          mn_id: mentor.mn_id,
       675          'Givebutter Contact ID': mentor.gb_contact_id?.toString() || null,
       676          'Contact External ID': mentor.mn_id,
       677 -        'Prefix': mentor.preferred_name || mentor.first_name,
       677 +        'Prefix': mentor.preferred_name,
       678          'First Name': mentor.first_name,
       679          'Middle Name': mentor.middle_name || null,
       680          'Last Name': mentor.last_name,

âº Bash(supabase migration new remove_display_name_from_mentors)
  âŽ¿ Â Created new migration at supabase/migrations/20251008212401_remove_display_name_from_mentors.sql

âº Write(supabase/migrations/20251008212401_remove_display_name_from_mentors.sql)
  âŽ¿ Â Error writing file

âº Read(supabase/migrations/20251008212401_remove_display_name_from_mentors.sql)
  âŽ¿ Â Read 1 line (ctrl+o to expand)

âº Write(supabase/migrations/20251008212401_remove_display_name_from_mentors.sql)
  âŽ¿ Â Wrote 9 lines to supabase/migrations/20251008212401_remove_display_name_from_mentors.sql
     -- Remove display_name column from mentors table (redundant with preferred_name)
     -- Make preferred_name NOT NULL since it's now always populated
     ALTER TABLE mentors
     DROP COLUMN IF EXISTS display_name;
     ALTER TABLE mentors
     ALTER COLUMN preferred_name SET NOT NULL;

âº Read(backend/core/etl/process.ts)
  âŽ¿ Â Read 25 lines (ctrl+o to expand)

âº Bash(supabase db reset)
  âŽ¿ Â Resetting local database...   
     Recreating database...
     Initialising schema...
     â€¦ +13 lines (ctrl+o to expand)

âº Fixed all the issues:

  1. Preferred name logic - Now always populated (uses prefix if specified, otherwise copies first_name). Every mentor has a preferred_name now.
  2. Removed display_name - Redundant with preferred_name. Created migration to drop the column and made preferred_name NOT NULL.
  3. Duplicate tracking - All duplicate signups (by phone) now logged to mn_errors with:
    - Error type: duplicate_signup
    - Severity: warning
    - Details about which was kept vs discarded
  4. Empty full_gb_contacts handling - This is expected behavior since we removed CSV import from baseline sync. The ETL handles it gracefully:
    - Mentors won't have gb_contact_id values until CSV is imported separately
    - This is by design - CSV import is now a separate feature
  5. campaign_joined_at - Already set to undefined (NULL in DB) because Givebutter API doesn't provide this data. This is correct.

  Database reset - Applied new migration removing display_name column.

  Ready to test! Run the sync from the web app again. You should now see:
  - All mentors with preferred_name populated
  - Duplicate signups logged in mn_errors
  - No gb_contact_ids (until CSV imported)
  - Clean separation between baseline sync and feature operations

> Ok so I have thought a lot about the csv export and I am realizing something after asking msyelf this: what is the point of it?\
At the end of the day, we are trying to do this with the full export\
- The only way to call a specific contact is with that contact id and I have noticed something: all campaign members are contacts but not all 
contacts are campaign members. When you fetch members which is what we are doing over API, you only get their member info, not their contact id. So, 
we have the user export the newest CSV any time they do a sync for these reasons\
--to collect and record the contact id for a mentor who had signed up, not been a contact and therefore had no contact id when we imported the 
prepared csv because we would have no other way to get the contact id for future imports to ensure contact merges and not duplication\
--to compare the mentors who have signed up since the last csv import(if the API config does not change) and appear as new mentors in our system but 
were mentors last year and therefore have contacts in gviebutter with information that needs to be updated for this year. This is alo where we would 
note the contact ids of any decided duplicates that deviate from a new mentors finalized single built out contact. Does this make sense?\
\
Given these are my thoughts of this scenario, I think that we need to slightly rethink the ways CSV imports and exports work. They are there because 
of a terrible givebutter API that does not allow us to search by any identifying info, does not allow us to add custom fields, etc. There are so many
 limitations that make these imports and exports necesicary but, it is important for us to figure out a way to seperate the core sync that will have 
to happen continously from the syncs that will happen manually with user input. FOr example, the initial sync will be one type of sync where new info
 is initialized. From there, any programatic API syncs possible will run on a schedule that will be configurable in the admin sync page. There will 
then be ways for users to prepare contact imports for giveutter, archive old duplicates, etc. These will be features that have predefined flows and 
are not synced all of the time. \
\
Given all of this, here is what I think we need to do:\
- We do not need to store the CSV given the only things/times its actually helpful is in specific feature flows like I spoke about above. Part of the
 flow would be uploading the latest CSVS which part of any CSV upload should be checking ALL potenital contacts that do not yet have a givebutter 
contact id stored but exist in our system/have been uploaded to givebutter\
-- I can think of needing a CSV any time we initialize new info for a new year, any time they want to send a text to mentors, any time they want to 
create a contact import to ensure all mentor contacts are up to date or exist depending on the case itself. Those are plently enough times for the 
logic we have introduced I believe because at its core our system doesnt care too much, that is why we are seperate from givebutter\
- Given this new approach above, there are certian things that will need to be changed like the running sync will have to be clear that it is jsut 
for the backend adn it runs at x interval. The flow itself will have t maek it clear and our documentation needs to understand this dance as well. I 
think this though should work and make sense moving forward. Lets really think this trugh before we make a plan 

âº You're thinking about this exactly right. Let me work through the logic with you:

  Current State Analysis

  Givebutter API Limitations:
  - Members API gives member_id but NOT contact_id
  - No way to search/query contacts by phone/email
  - No way to update custom fields via API
  - CSV export is the ONLY way to get contact_ids and full contact data

  Your Key Insight:
  Campaign members âŠ† Contacts. You can have contacts who never became campaign members (signed up but didn't create page), but every member is a
  contact. The API only exposes the member view, not the contact ID needed for operations.

  Why CSV is Actually Needed

  Let me trace through specific scenarios:

  Scenario 1: New mentor, first year
  1. Signs up via Jotform â†’ we get their info
  2. Creates Givebutter page â†’ becomes campaign member
  3. API sync gets member_id, amount raised
  4. But no contact_id â†’ can't update their contact custom fields
  5. CSV upload â†’ match by phone/email â†’ get contact_id â†’ store on mentor record
  6. Now we can generate import CSVs with their contact_id for updates

  Scenario 2: Returning mentor
  1. Mentored last year â†’ has Givebutter contact from 2024
  2. Signs up for 2025 â†’ new Jotform submission
  3. Shows up as "new" in our system
  4. CSV upload â†’ detects existing contact with same phone from 2024
  5. Match â†’ update gb_contact_id â†’ flag as returning mentor
  6. Their 2024 contact info can be updated with 2025 details

  Scenario 3: Duplicate contacts in Givebutter
  1. Person exists 3 times in Givebutter (email typos, multiple signups, etc.)
  2. API doesn't expose this - only shows if they're a campaign member
  3. CSV upload â†’ detect all 3 contact_ids for same phone
  4. Log to mn_errors as duplicates needing manual merge
  5. Feature: archive old duplicates, keep canonical one

  Your Proposed Architecture (Validated)

  Tier 1: Automated Baseline Sync (No CSV, API only)
  - Runs on schedule (configurable: hourly, daily, etc.)
  - Jotform signups â†’ mn_signups_raw
  - Jotform setup â†’ funds_setup_raw
  - Givebutter members â†’ campaign_members_raw
  - ETL â†’ mentors, mn_tasks, mn_errors, mn_gb_import
  - Works perfectly fine WITHOUT gb_contact_id values
  - Purpose: "Here's what we know from all mentor sources"

  Tier 2: Manual CSV Operations (User-initiated)
  - Upload fresh Givebutter CSV as part of feature flow
  - Parse â†’ temporarily populate full_gb_contacts
  - Match contacts to mentors (phone/email)
  - Update mentors.gb_contact_id where matched
  - Detect duplicates â†’ log to mn_errors
  - Clear CSV data after operation? (or keep latest)

  Tier 3: Export Operations (Push to Givebutter)
  - Use mn_gb_import table (always fresh from ETL)
  - User downloads CSV â†’ imports to Givebutter
  - Updates contact custom fields, tags, etc.
  - Requires contact_ids from Tier 2

  When CSV Upload is Required

  1. Initial year setup - Match all existing contacts from previous year
  2. Pre-text campaign - Need contact_ids to generate proper export
  3. Contact sync operation - Update Givebutter with latest mentor info
  4. Duplicate cleanup - Identify and archive old contacts
  5. Returning mentor detection - Find mentors from previous years

  What Changes Are Needed

  Database:
  - full_gb_contacts becomes temporary/operational (not continuously synced) âœ“ Already done
  - Keep it in DB but understand it's only populated during manual operations
  - Could add last_csv_import_at timestamp to track freshness

  API Endpoints:
  - /api/sync/run - Baseline sync (API only) âœ“ Already correct
  - /api/sync/upload-csv - Should also match to mentors and update gb_contact_id
  - New: /api/features/* - CSV-dependent operations (text, export, duplicates)

  UI/UX:
  - Settings page needs clear sections:
    - Automated Sync (API config, interval, last run)
    - Manual Operations (require CSV upload first)
  - Each feature flow starts with: "Upload latest Givebutter CSV"
  - Show warnings if CSV is stale/missing for manual operations

  Documentation:
  - Two-tier sync model
  - When CSV is needed vs not needed
  - How contact_id matching works

  Potential Issues & Solutions

  Issue 1: gb_contact_id staleness
  - Contact_ids from 2024 might be invalid if contact was deleted
  - Solution: Every manual operation starts with fresh CSV upload
  - Validates contact_ids are still current

  Issue 2: Partial contact_id coverage
  - Some mentors will have gb_contact_id, others won't
  - Solution: This is expected! Only mentors who've been matched via CSV have it
  - mn_gb_import handles both cases (with/without contact_id)

  Issue 3: When to clear full_gb_contacts?
  - Option A: Clear after each operation
  - Option B: Keep latest until next upload
  - Recommendation: Keep latest, show upload timestamp, let user re-upload anytime

  Issue 4: Duplicate detection only works with CSV
  - Givebutter duplicates invisible to API
  - Solution: This is fine - duplicate cleanup is manual feature anyway
  - Document that duplicate detection requires CSV upload

  Is This Architecture Sound?

  Yes, for these reasons:

  1. Separation of concerns
    - Automated sync: Keep internal state fresh
    - Manual operations: Interface with Givebutter contacts
    - Clean, understandable boundary
  2. Handles API limitations
    - Works around lack of contact_id in members API
    - Uses CSV only when actually needed
    - Doesn't force CSV into baseline sync
  3. Scalable
    - Automated sync can run frequently (every hour)
    - CSV operations only when user needs specific feature
    - No unnecessary CSV storage/processing
  4. Matches mental model
    - "Our system tracks mentors from all sources"
    - "Givebutter integration requires CSV for contact operations"
    - Clear when each tier applies
  5. Graceful degradation
    - Works perfectly without CSV (baseline sync)
    - Additional features unlock when CSV provided
    - Never breaks if CSV is stale

  Recommended Implementation Plan

  Would you like me to proceed with refactoring based on this architecture? Here's what I would do:

  1. Update /api/sync/upload-csv to match contacts to mentors
  2. Add contact_id matching logic (phone â†’ email fallback)
  3. Create clear UI separation in settings (automated vs manual)
  4. Document the two-tier sync model
  5. Build feature flows that start with CSV upload requirement

  Does this analysis match your thinking? Any gaps or concerns?

> Ok so I am going to provide my comments sequentially as I read down the page:\
All of your current state analysis is GREAT you are right. There is one senario that is missing tho. When we havea  new mentor that did not 
participate last year, we did the manual CSV steps to prepare a contact sync import, and their contact has synce been synced but, we do not have 
their contact id yet. That will come after we upload the csv again. This might be clear but I wanted to note.\
maybe lets make the API api/sync/features to keep the sync features seperate from other API features we will need to add\
For the changes\
- yes lets store it but note the staleness\
- in the UI UX, the only thing that will be in the settings pane will be the first time sync config like we already have it with the necessicary 
edits. The future text message sync functionality will be housed in the Comms page and the mentor gb contact import sync will be help in the mentors 
tab. The sync tab will only be used to monitor all syncs, look through errors, configure the automatic, etc. We will have to workshop this a little 
bit more but this is a great starting point.\
\
This is a HUGE imporvement in the thinking and logic behind this system. Your response was honestly quite perfect and I want to make sure we record 
these edge cases & the complex logic to ensure we are creating a locked down system. Lets finalize the plans and move forward. 

âº Perfect! Let me incorporate your feedback and finalize the plan.

  Additional Scenario (CSV Feedback Loop)

  Scenario 5: New mentor â†’ CSV export â†’ Givebutter import â†’ CSV re-upload
  1. New mentor signs up â†’ no Givebutter contact exists
  2. User generates import CSV from mn_gb_import â†’ includes mentor without contact_id
  3. User uploads CSV to Givebutter â†’ Givebutter creates new contact, assigns contact_id
  4. User downloads fresh Givebutter CSV â†’ uploads back to our system
  5. Matching logic finds newly created contact by phone/email â†’ captures contact_id
  6. Mentor record now complete with gb_contact_id for future updates

  This is a critical feedback loop that closes the gap between our system and Givebutter.

  Refined Architecture

  API Structure (/api/sync/features/*)

  /api/sync/
    â”œâ”€ run                    (POST) Automated baseline sync
    â”œâ”€ config                 (GET/POST) API configuration
    â”œâ”€ test-keys              (POST) Validate API keys
    â”œâ”€ discover-jotform       (POST) Discover forms
    â”œâ”€ discover-givebutter    (POST) Discover campaigns
    â””â”€ features/
        â”œâ”€ csv-upload         (POST) Upload & match CSV â†’ update gb_contact_ids
        â”œâ”€ csv-status         (GET) CSV freshness & coverage stats
        â”œâ”€ contact-export     (POST) Generate Givebutter import CSV
        â”œâ”€ text-export        (POST) Generate SMS campaign export
        â””â”€ duplicates         (GET) List duplicate contacts needing merge

  Page Organization

  Settings Page (/settings)
  - Purpose: Initial setup only
  - API Configuration (Jotform + Givebutter keys, form/campaign discovery)
  - Automated Sync Configuration (enable, interval, schedule)

  Sync Page (/sync)
  - Purpose: Monitoring & control center
  - Sync log viewer (all sync types - automated, manual, CSV uploads)
  - Error log viewer (mn_errors table with filtering)
  - Manual baseline sync trigger
  - CSV import history & freshness indicator
  - Sync statistics dashboard

  Mentors Page (/mentors)
  - Purpose: Mentor management + Givebutter contact operations
  - Mentor list/table
  - CSV Upload widget (with freshness warning)
  - "Generate Contact Import CSV" button
  - "Detect Duplicates" viewer
  - CSV status: "Last uploaded X hours ago, Y/Z mentors have contact_ids"

  Communications Page (/communications)
  - Purpose: Text message campaigns
  - Campaign builder/scheduler
  - CSV Upload widget (with freshness warning)
  - "Generate Text Export" button
  - Recipient filtering (by status, shift, etc.)
  - CSV status indicator

  Database Changes

  full_gb_contacts table:
  - Add csv_uploaded_at TIMESTAMPTZ - track upload freshness
  - Add csv_filename TEXT - track source file
  - Keep data between uploads (don't clear)

  New table: csv_import_log
  CREATE TABLE csv_import_log (
    id SERIAL PRIMARY KEY,
    uploaded_at TIMESTAMPTZ DEFAULT NOW(),
    filename TEXT,
    total_contacts INTEGER,
    mentors_matched INTEGER,
    new_contact_ids_captured INTEGER,
    duplicates_detected INTEGER,
    uploaded_by TEXT -- future: user auth
  );

  CSV Upload & Matching Logic

  When user uploads Givebutter CSV:

  1. Parse & Store
    - Parse CSV â†’ validate columns
    - Upsert to full_gb_contacts (by contact_id)
    - Record upload timestamp
  2. Match to Mentors (priority order)
    - Primary: Normalize phone â†’ match mentors.phone
    - Secondary: Normalize email â†’ match personal or UGA email
    - Tertiary: Match campaign_members_raw.member_id â†’ join to mentor
  3. Update Mentors
    - Where matched â†’ UPDATE mentors SET gb_contact_id = X WHERE mn_id = Y
    - Track count of new contact_ids captured
  4. Detect Duplicates
    - Group contacts by phone â†’ if count > 1, log to mn_errors
    - Group contacts by email â†’ if count > 1, log to mn_errors
    - Include all contact_ids in error details
  5. Return Summary
  {
    "contacts_imported": 1234,
    "mentors_matched": 450,
    "new_contact_ids": 87,
    "duplicates_detected": 12,
    "csv_age_hours": 0.5
  }

  Edge Cases & Handling

  | Edge Case          | Scenario                                 | Solution                                                |
  |--------------------|------------------------------------------|---------------------------------------------------------|
  | Phone changed      | Old Jotform phone, updated in Givebutter | Fallback to email match, log warning                    |
  | Shared phone       | Family members using same number         | Match by email if ambiguous, log to errors              |
  | Deleted contact    | gb_contact_id exists but contact gone    | Log error "Stale contact_id", don't clear automatically |
  | Stale CSV          | CSV older than latest signups            | Show age warning, recommend fresh upload                |
  | Duplicate contacts | Same person multiple times in GB         | Log all contact_ids to mn_errors for manual resolution  |
  | Partial CSV        | User uploads subset of contacts          | Only update/add, never clear existing contact_ids       |
  | No match found     | Contact in CSV but not in our system     | Ignore (expected - other campaigns, donors, etc.)       |
  | Multiple matches   | One contact matches multiple mentors     | Log error, require manual resolution                    |

  CSV Freshness Indicators

  Show in UI:
  - Last uploaded: "2 hours ago" or "3 days ago" (with warning if > 24h)
  - Coverage: "450/555 mentors have contact_ids (81%)"
  - Status badge: ðŸŸ¢ Fresh | ðŸŸ¡ Stale | ðŸ”´ Missing

  When to warn:
  - CSV > 24 hours old for text campaigns
  - CSV > 7 days old for contact imports
  - CSV missing entirely for any CSV-dependent feature

  Documentation Requirements

  User Documentation:
  1. Two-tier sync model (automated vs manual)
  2. When CSV upload is required
  3. CSV feedback loop workflow
  4. How contact_id matching works
  5. Duplicate detection & resolution

  Technical Documentation:
  1. Complete scenario flowcharts (all 5 scenarios)
  2. Matching algorithm logic
  3. Edge case handling
  4. API endpoint specifications
  5. Database schema & relationships

  Implementation Order

  1. Phase 1: CSV Upload & Matching
    - Update /api/sync/features/csv-upload endpoint
    - Implement matching logic (phone â†’ email â†’ member_id)
    - Update mentors with gb_contact_ids
    - Duplicate detection â†’ mn_errors
  2. Phase 2: CSV Status & Monitoring
    - Add csv_uploaded_at to full_gb_contacts
    - Create csv_import_log table
    - /api/sync/features/csv-status endpoint
    - Freshness calculations & warnings
  3. Phase 3: UI Components
    - CSV upload widget (reusable across pages)
    - CSV status indicator component
    - Update Sync page with logs
    - Add widgets to Mentors & Comms pages
  4. Phase 4: Export Features
    - Contact import CSV generation
    - Text campaign export
    - Duplicate viewer UI
  5. Phase 5: Documentation
    - User guide with scenarios
    - Technical architecture docs
    - Edge case reference

  Does this capture everything correctly? Ready to proceed with implementation?

> Yes this all literally looks amazing. One last thing that I thought about, there is a clear seperation for the plan for the difference between the 
sync button/automatic sync that is tracked on the sync page and the initial setup/initialization. The setup itself will need to call some featurs to 
fully initialize the contacts which will need to be different from the periodic sync. You might know this I just need to ensure that that is the 
case. 

âº Excellent catch! You're absolutely right - there are actually THREE distinct flows, not two:

  Flow Breakdown

  1. Initial Setup/Initialization (One-time)

  Location: Settings page â†’ "Initialize System" wizard
  Trigger: First-time setup or new year initialization

  Steps:
  1. Configure API keys (Jotform + Givebutter)
  2. Discover & select forms/campaigns
  3. Run Initial Sync â†’ baseline API sync + ETL
  4. Upload Givebutter CSV (required, not optional)
  5. Run Contact Matching â†’ populate gb_contact_ids
  6. Show Initialization Summary:
    - X mentors imported
    - Y contact_ids matched
    - Z duplicates detected
    - Status: âœ… System Initialized

  Purpose: Bootstrap the system with historical data and establish contact_id mappings for all existing mentors from previous years.

  2. Periodic/Automated Sync (Ongoing)

  Location: Runs automatically OR manual trigger from Sync page
  Trigger: Schedule (configurable: hourly/daily) OR "Run Sync Now" button

  Steps:
  1. Fetch Jotform signups â†’ mn_signups_raw
  2. Fetch Jotform setup â†’ funds_setup_raw
  3. Fetch Givebutter members â†’ campaign_members_raw
  4. Run ETL â†’ update mentors, mn_tasks, mn_errors, mn_gb_import
  5. Does NOT touch CSV or contact_ids

  Purpose: Keep internal state fresh from API sources. Track new signups, completed tasks, fundraising progress.

  3. Manual Feature Operations (User-initiated)

  Location: Mentors page, Communications page
  Trigger: User needs CSV-dependent functionality

  Examples:
  - Upload fresh CSV â†’ update contact_ids (captures new contacts created in Givebutter)
  - Generate contact import CSV
  - Generate text campaign export
  - View/resolve duplicates

  Purpose: Interface with Givebutter contacts when needed for specific operations.

  Key Distinctions

  | Aspect            | Initial Setup     | Periodic Sync       | Feature Operations    |
  |-------------------|-------------------|---------------------|-----------------------|
  | Frequency         | Once per year     | Hourly/Daily        | As needed             |
  | CSV Required?     | âœ… Yes (mandatory) | âŒ No                | âœ… Yes (per operation) |
  | Contact Matching? | âœ… Yes             | âŒ No                | âœ… Yes (on upload)     |
  | Purpose           | Bootstrap system  | Keep data fresh     | Specific tasks        |
  | Logged As         | initialization    | automated or manual | feature_csv_upload    |

  Implementation Impact

  Settings Page UI Flow

  Before Initialization:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ âš ï¸  System Not Initialized          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 1. Configure API Keys               â”‚
  â”‚    [Jotform] [Givebutter]           â”‚
  â”‚                                     â”‚
  â”‚ 2. [Initialize System] â† Big button â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Initialize System Flow (Multi-step):
  Step 1: API Sync
    â”œâ”€ Fetching Jotform signups... âœ“
    â”œâ”€ Fetching Jotform setup... âœ“
    â”œâ”€ Fetching Givebutter members... âœ“
    â””â”€ Running ETL... âœ“
        â†’ 555 mentors imported

  Step 2: CSV Upload (Required)
    â”œâ”€ [Upload Givebutter Contacts CSV]
    â””â”€ âš ï¸  Required to complete initialization

  Step 3: Contact Matching
    â”œâ”€ Matching contacts to mentors...
    â”œâ”€ Phone matches: 450
    â”œâ”€ Email matches: 87
    â””â”€ Duplicates detected: 12
        â†’ 537/555 mentors have contact_ids (97%)

  âœ… Initialization Complete!

  After Initialization:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ âœ… System Initialized                â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Last initialized: Jan 15, 2025      â”‚
  â”‚ Coverage: 537/555 contacts (97%)    â”‚
  â”‚                                     â”‚
  â”‚ Automated Sync: [Enabled]           â”‚
  â”‚ Interval: Every 6 hours             â”‚
  â”‚ Last run: 2 hours ago               â”‚
  â”‚                                     â”‚
  â”‚ [Run Manual Sync] [Re-initialize]   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  API Endpoint Structure

  /api/sync/
    â”œâ”€ initialize              (POST) Initial setup flow
    â”‚   â””â”€ Runs baseline sync + prompts for CSV + matching
    â”‚
    â”œâ”€ run                     (POST) Periodic baseline sync ONLY
    â”‚   â””â”€ No CSV operations
    â”‚
    â””â”€ features/
        â”œâ”€ csv-upload         (POST) Upload & match CSV
        â”œâ”€ csv-status         (GET) CSV freshness stats
        â”œâ”€ contact-export     (POST) Generate import CSV
        â””â”€ text-export        (POST) Generate SMS export

  Sync Log Types

  The sync_log table should track different types:

  type SyncType =
    | 'initialization'        // Initial setup with CSV
    | 'automated'            // Scheduled baseline sync
    | 'manual'               // User-triggered baseline sync
    | 'feature_csv_upload'   // CSV upload operation
    | 'feature_export'       // Export generation

  Does This Capture It?

  So to confirm:
  - Initial setup = special flow that includes CSV upload as mandatory step
  - Periodic sync = just API + ETL, no CSV
  - "Run Sync Now" button on Sync page = triggers periodic sync (no CSV)
  - CSV uploads after initialization = part of feature flows, not syncs

  Is this the distinction you were looking for?

> Yes that is all great. I think we are there with the plan, is there anything else we are missing? 

âº Let me do a final checklist to catch any gaps:

  Quick Audit

  âœ… Covered Thoroughly:

  - Three-tier sync architecture (init, periodic, features)
  - CSV matching logic and feedback loop
  - All 5 scenarios + edge cases
  - Page organization and UI flows
  - API endpoint structure
  - Database schema additions

  â“ Need to Clarify:

  1. mn_gb_import Table Lifecycle
  - Currently: Gets regenerated during every ETL run
  - Question: Should it be cleared and rebuilt each time, or upserted?
  - What happens to rows for mentors who are later removed?

  2. Contact Export Download
  - User needs to download mn_gb_import as CSV
  - Is this /api/sync/features/contact-export?
  - Format: exact Givebutter import CSV spec?
  - Should it only include mentors with certain statuses/filters?

  3. Re-initialization Semantics
  - What does "Re-initialize" button actually do?
    - Option A: Clear all tables + run init flow again (nuclear)
    - Option B: Re-run ETL + re-match CSV (soft refresh)
    - Option C: Just for new year setup (2025 â†’ 2026 transition)

  4. Duplicate Resolution Workflow
  - We detect duplicates â†’ log to mn_errors
  - How does user actually resolve them?
    - Just viewing in errors log?
    - Built-in "Archive Duplicate" feature?
    - Manual process in Givebutter + mark as resolved in our system?

  5. Schedule Configuration Storage
  - User sets "Every 6 hours" for automated sync
  - Where is this stored?
    - Add to sync_config table?
    - New scheduled_jobs table?
    - How is it actually triggered (cron, polling, Next.js cron)?

  6. Error Notifications
  - Automated sync fails at 3am
  - How does user know?
    - Just check Sync page manually?
    - Email alerts (future)?
    - For now: just log to sync_log with status='failed'?

  7. Text Campaign Export Details
  - What format does SMS platform need?
  - Just phone numbers, or full contact data?
  - Filtering options (by status, shift, etc.)?
  - This can be Phase 2, but worth noting

  ðŸ” Migration Consideration

  You mentioned you already have data from previous testing. When we implement this:
  - Do you need to run initialization flow?
  - Or can you just upload CSV to complete current state?
  - Should we build a "complete initialization" endpoint for current state?

  My Questions

  1. mn_gb_import: Clear & rebuild every ETL, or upsert? Include all mentors or only certain statuses?
  2. Re-initialize: What should it actually do? When would you use it vs just uploading new CSV?
  3. Duplicate resolution: Just view in errors log, or build resolution UI?
  4. Scheduling: How should automated sync be triggered? Simple interval or cron-like?
  5. Current migration: Do you need to initialize from current state, or start fresh?

  Are these important to nail down now, or can some be "Phase 2" features?

> 1. I am honestly not sure...my ituition is to say just obliterate each time ine the mentor table is the source of truth. Wait, I just thought of 
another edge case. People will have the ability to edit contact info from the givebutter web app so, wehn the CSV is uploaded for contact syncs, we 
need to make sure that the info from the export (containting the changes from the web UI we cannot fetch) is added to the mentor table correctly. The
 difficulty will be deciding when there are conflicts which is correct but, I think it will be date based in most cases? What do you think?\
2. This is an important distinction, the CSV imports happen when we prepare the mn_gb_imports for export from our system for syncing with givebutter.
 When we export a CSV, we will then import it to the we ui and sync contacts. When we are talking about CSV exports, that is the full export coming 
from givebutter which will then be imported to our system. The point of this is to fully sync each contact with full fields (making our way around 
the API limitations)\
3. For context, the whole point of our org is to have an event day each novemeber. We are at a uni so we recruit first in the spring semester then 
finish in the fall before event day. The "initialization" each year will likley not happen till end of summer, early fall each year. When this 
reinitialization happens, we are ensuring the API keys are up to date, the forms are mapped correctly, etc. This wont have to happen often and hence 
this is why it is in the settings\
4. Any duplicates we find will then be reviewed and manually approved then, we will duplicate the excess contacts with their contact ID via API\
5. The frequency will be set from the sync page. Maybe store it in the sync_config? Do we need a scheduled jobs table> I have no clue how we would 
actually triggger it\
6. We will add this in later no worries for now\
7. We will do this later\
\
I am using a supabase instance that is dockerized for dev so we can just reset the db adn start again once this is all done\
\
Given this what are your thughts 

âº Perfect, this clarifies a lot. Let me address the big one:

  1. CSV Upload Sync-Back (Critical Design Decision)

  This is a two-way sync problem that needs careful thought:

  The Scenario

  Jotform (source) â†’ Our System â†’ Export CSV â†’ Givebutter Import
                                                      â†“
                                              Admin edits in GB UI
                                                      â†“
  Admin downloads GB CSV â†’ Upload to our system â†’ Update mentors table?

  What Fields Might Be Edited in Givebutter?

  - Contact corrections: Phone/email typos fixed by admin
  - Name corrections: Preferred name updated, legal name fixed
  - Custom fields: Status notes, shift changes, etc.
  - Tags: Additional tags added manually

  The Conflict Problem

  Example:
  - Jotform signup: phone = "(555) 123-4567", name = "Rob"
  - Mentor table: phone = "+15551234567", preferred_name = "Rob"
  - Admin notices it's wrong, edits in GB: phone = "(555) 999-8888", name = "Robert"
  - CSV uploaded back to our system
  - What should happen?

  My Recommendation: Tiered Field Ownership

  | Field Category                         | Source of Truth | CSV Upload Behavior                |
  |----------------------------------------|-----------------|------------------------------------|
  | Identity (mn_id, signup_submission_id) | Jotform         | Never update                       |
  | Primary Contact (phone, uga_email)     | Jotform         | Log discrepancy, don't update      |
  | Secondary Contact (personal_email)     | Either          | Update if changed in GB            |
  | Names (first, middle, last)            | Jotform         | Log discrepancy, don't update      |
  | Preferred Name (prefix)                | Either          | Update from GB (admin may correct) |
  | Demographics (gender, shirt, class)    | Jotform         | Don't update                       |
  | Custom Fields                          | Derived         | Don't sync back (one-way export)   |
  | Tags                                   | Both            | Merge (keep ours + add GB tags)    |
  | gb_contact_id                          | Givebutter      | Always update (this is the point!) |

  Proposed Logic for CSV Upload

  for each contact in CSV:
    match = find_mentor(contact.phone || contact.email)

    if match:
      // Always update contact ID
      UPDATE gb_contact_id

      // Check for discrepancies
      if contact.phone != match.phone:
        LOG_ERROR('phone_mismatch', severity='warning')

      if contact.email != match.personal_email:
        // Allow personal email updates from GB
        UPDATE personal_email

      if contact.prefix != match.preferred_name:
        // Allow preferred name corrections from GB
        UPDATE preferred_name
        LOG_INFO('preferred_name_updated_from_gb')

      // Custom fields: ignore (don't sync back)
      // Tags: merge
      if contact.tags:
        MERGE tags (keep ours, add new from GB)

  Alternative: "GB Override Mode"

  Add a setting: sync_config.allow_gb_overrides: boolean

  - false (default): Only update gb_contact_id, log discrepancies
  - true: Allow GB to update name/contact fields

  This gives you control over trust level.

  What Do You Think?

  Should we:
  - Option A: Only update gb_contact_id, log all other discrepancies for manual review
  - Option B: Allow GB to override certain fields (preferred name, personal email)
  - Option C: Add "GB Override Mode" toggle

  I lean toward Option B - update gb_contact_id + preferred_name + personal_email, but protect core Jotform fields.

  ---
  Other Answers Look Good

  2. CSV terminology - âœ… Clear now

  3. Re-initialization - Annual flow, makes sense in Settings

  4. Duplicates - View â†’ Manual review â†’ API archive (future feature)

  5. Scheduling:
  ALTER TABLE sync_config ADD COLUMN automated_sync_enabled BOOLEAN DEFAULT false;
  ALTER TABLE sync_config ADD COLUMN sync_interval_hours INTEGER DEFAULT 6;
  - Store config in sync_config
  - Manual trigger for now
  - Actual scheduler in Phase 2 (could use Vercel Cron, pg_cron, or external service)

  6 & 7 - Phase 2 âœ…

  DB reset - âœ… Start fresh

  Ready to Finalize?

  Once you decide on the CSV sync-back strategy (Option A/B/C), I think we're ready to lock in the plan. What's your preference?

> Hoenstly I might have introduced some complexity with that point. There are very few occasions where anything identifying will change (the most 
likely changes will be prefferd name which isprefix on givebutter and maybe the email fields in case someone wants a different primary email). 
Outside of that, the only otehr things that might get changed are the custom fields which we DO need to honor in any case (like lets say our system 
says a mentor didnt do something, an admin in givebutter was told they did, updates the custom fields, which will then be included in he CSV import).
 However, I just had a thought. Given this info https://docs.givebutter.com/reference/get-contact once we have a givebutter contact ID, we should be 
able to fetch the contact for syncing. You cannot update most parts of a contact via API but, you can recieve all the info (including when last 
updated). The reason this is important because the CSV imports will be used to find givebutter IDs then, the givebutter ids will sync with our 
mn_gb_contacts table with the contactID and get contact API avility. We can add this functionality into the periodic sync so the users in our system 
will know "Not synced with givebutter currently, please reimport updated contacts after resolving conflicts" or something along those lines. This 
way, we wont introduce the possibility of info getting merged incorrectly. \
\
I think this might be the last link to our system. This way the only reason we need the CSV is for the ids, all the other info we can pull and sync 
realtime. Thsi will mean we must upsert to the mn_gb_table thught. I apolagize for the back and forth but what do you think given this 

âº ðŸŽ¯ This is BRILLIANT! You just solved the entire two-way sync problem elegantly.

  The Realization

  You're absolutely right - once we have gb_contact_id, we can use the API to fetch everything:

  GET /contacts/{contact_id}
  Response includes:
  - All contact fields (name, phone, email)
  - Custom fields (with our custom field values!)
  - Tags
  - Last updated timestamp
  - Everything except things we can't set via API

  New Architecture (Much Cleaner!)

  CSV Purpose: ID Capture Only

  CSV Upload Flow:
  1. Parse CSV
  2. Match contacts to mentors (phone/email)
  3. Extract contact_id â†’ UPDATE mentors.gb_contact_id
  4. Done! (Don't store other CSV data)

  Periodic Sync: Full Data Sync via API

  Periodic Sync Flow:
  1. Jotform API â†’ mn_signups_raw, funds_setup_raw
  2. Givebutter API â†’ campaign_members_raw
  3. ETL â†’ mentors, mn_tasks, mn_errors, mn_gb_import

  4. Contact Sync (NEW!):
     FOR EACH mentor WHERE gb_contact_id IS NOT NULL:
       - GET /contacts/{gb_contact_id}
       - UPSERT to mn_gb_contacts (full contact data)
       - Compare custom fields:
         - If GB has updates â†’ sync back to mentor
       - Check for conflicts:
         - If phone/email/name differs â†’ log to mn_errors
       
  5. Update mentor sync status

  mn_gb_contacts Table: API-Synced Cache

  CREATE TABLE mn_gb_contacts (
    contact_id INTEGER PRIMARY KEY,
    mn_id TEXT REFERENCES mentors(mn_id), -- Link to our mentor

    -- Contact data from API
    first_name TEXT,
    last_name TEXT,
    primary_email TEXT,
    primary_phone TEXT,
    tags TEXT[],
    custom_fields JSONB,  -- All custom fields from GB

    -- Sync metadata
    last_synced_at TIMESTAMPTZ,
    gb_updated_at TIMESTAMPTZ,  -- From API response
    sync_status TEXT,  -- 'synced' | 'conflict' | 'stale'

    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
  );

  Conflict Detection Strategy

  During periodic sync with contact API:

  const contactFromAPI = await fetch(`/contacts/${gb_contact_id}`)
  const mentor = getMentor(mn_id)

  // Check for conflicts
  const conflicts = []

  if (normalizePhone(contactFromAPI.phone) !== mentor.phone) {
    conflicts.push({
      field: 'phone',
      jotform: mentor.phone,
      givebutter: contactFromAPI.phone,
      gb_updated_at: contactFromAPI.updated_at
    })
  }

  if (normalizeEmail(contactFromAPI.email) !== mentor.personal_email) {
    conflicts.push({
      field: 'personal_email',
      jotform: mentor.personal_email,
      givebutter: contactFromAPI.email,
      gb_updated_at: contactFromAPI.updated_at
    })
  }

  if (contactFromAPI.custom_fields['Preferred Name'] !== mentor.preferred_name) {
    // This is actually GOOD - admin corrected it in GB
    // Sync it back!
    UPDATE mentor.preferred_name = contactFromAPI.custom_fields['Preferred Name']
  }

  if (conflicts.length > 0) {
    // Log to mn_errors
    INSERT INTO mn_errors {
      mn_id,
      error_type: 'contact_data_conflict',
      severity: 'warning',
      error_message: 'Contact data differs between Jotform and Givebutter',
      raw_data: { conflicts, contact_id: gb_contact_id }
    }

    // Update sync status
    UPDATE mn_gb_contacts SET sync_status = 'conflict'
    UPDATE mentors SET gb_sync_status = 'needs_review'
  }

  UI Indicators

  Mentor Table:
  | MN ID | Name  | Status      | GB Sync               |
  |-------|-------|-------------|-----------------------|
  | 1001  | Alice | Complete    | âœ… Synced 2h ago       |
  | 1002  | Bob   | Needs Page  | âš ï¸ Conflicts (review) |
  | 1003  | Carol | Needs Setup | ðŸ”„ Syncing...         |
  | 1004  | Dave  | Complete    | âŒ No contact ID       |

  Sync Page:
  Last Contact Sync: 2 hours ago
  â”œâ”€ 450 contacts synced successfully
  â”œâ”€ 12 conflicts detected (needs review)
  â””â”€ 105 mentors without contact IDs

  Rate Limiting Consideration

  If you have 555 mentors with contact IDs, that's 555 API calls during each periodic sync.

  Options:
  1. Batch with delay: Fetch 10 contacts, wait 1 second, repeat (slower but safer)
  2. Separate schedule: Sync contacts every 24h, not every 6h
  3. Incremental: Only sync contacts where gb_updated_at > last_synced_at (need to check if API provides this)
  4. On-demand: Only sync when user views mentor details

  Recommendation: Start with option 2 - contacts change less frequently than signups/members.

  CSV Upload Simplified

  Before (complex):
  - Upload CSV
  - Parse all fields
  - Store to full_gb_contacts
  - Match to mentors
  - Sync data back
  - Detect duplicates

  After (simple):
  - Upload CSV
  - Parse contact_id + phone/email (matching fields only)
  - Match to mentors by phone/email
  - Update mentors.gb_contact_id
  - Done! API handles the rest

  Benefits of This Approach

  âœ… Single source of truth: Jotform for identity, API for current GB state
  âœ… Real-time sync: Custom field updates from GB flow back automaticallyâœ… Conflict detection: Automated, logged, reviewable
  âœ… Simpler CSV logic: Just ID capture, no data merging
  âœ… Better UX: Users see sync status, know when data is fresh
  âœ… Scalable: Add more contact fields without changing CSV logic

  Updated Implementation Plan

  Phase 1: (What we'll build now)
  1. CSV upload â†’ contact ID extraction only
  2. Periodic sync â†’ add contact API fetching step
  3. mn_gb_contacts â†’ upserted from API, not CSV
  4. Conflict detection â†’ log to mn_errors
  5. Sync status indicators in UI

  Phase 2: (Later)
  - Conflict resolution UI
  - Selective contact sync (only changed)
  - Contact sync scheduling config

  Does This Match Your Vision?

  This is WAY cleaner than CSV merging logic. The CSV becomes a simple "contact ID discovery" tool, and everything else is API-driven.

  Should we lock this in and build it?

> Well yes this is correct for everything except for the first upload. We still need to be able to check the CSV for mentors who were there last year
 when we are first addint them to our system so we can note their gb contact ids (or recognize there are multiple contacts). given this, we will 
still need much of the functionality if only for this one purpose however, when doing gb contact imports (when preparing them) we can be sure the 
data is not stale given the API. Also, we are still going to ahve to store this full list of emails from the export csv given this. \
\
Also, maybe we should introduce a raw mn gb contacts table or something so we can have a table that  should be the EXACT format as the response from 
the API so we can keep a copy of what givebutter looks like each time we sync via API to compare against our mentor table (and notify the admins of 
any conflicts). This way, we have a raw table to recheck or retry depending on the scenario, we have a raw table to ensure we are always fully synced
 with each mentor gb contact which will then be used to prepare the gb import csv (with the table with the EXACT fields needed for the import). My 
thought was that we can relationally connect the raw sync table with the mn id(easy to relate to the main mentor table) then relate the raw table to 
the mn import table. This way, we will avoid all and any hard coded logic for contact merges (which will be unlikley outside of the custom fields 
which will change. However, it should be noted that amount fundraised will be collected when campaign members are grabbed. Does this make sense? 

âº YES! This is much clearer architecture. Let me map it out:

  Three-Table Strategy

  1. mn_gb_contacts_raw - What Givebutter Currently Has

  CREATE TABLE mn_gb_contacts_raw (
    contact_id INTEGER PRIMARY KEY,
    mn_id TEXT REFERENCES mentors(mn_id),  -- Linked after matching

    -- Exact API response structure
    first_name TEXT,
    last_name TEXT,
    prefix TEXT,
    primary_email TEXT,
    primary_phone TEXT,
    email_addresses TEXT[],
    phone_numbers TEXT[],
    tags TEXT[],
    custom_fields JSONB,  -- Exact structure from GB

    -- Metadata
    source TEXT,  -- 'csv_upload' | 'api_sync'
    gb_updated_at TIMESTAMPTZ,
    last_synced_at TIMESTAMPTZ DEFAULT NOW(),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
  );

  Purpose: Mirror of Givebutter's current state. Used for:
  - Conflict detection (compare against mentors table)
  - Preparing exports (know what GB already has)
  - Historical tracking (what did GB look like when we last synced?)

  2. mn_gb_import - What We're Preparing to Push

  -- This table already exists, but clarify its purpose
  CREATE TABLE mn_gb_import (
    mn_id TEXT PRIMARY KEY REFERENCES mentors(mn_id),

    -- Exact Givebutter CSV import format
    'Givebutter Contact ID' TEXT,
    'Contact External ID' TEXT,
    'Prefix' TEXT,
    'First Name' TEXT,
    'Middle Name' TEXT,
    'Last Name' TEXT,
    'Primary Email' TEXT,
    'Primary Phone Number' TEXT,
    'Email Addresses' TEXT,
    'Phone Numbers' TEXT,
    'Tags' TEXT,
    'Gender' TEXT,
    -- ... all custom fields

    needs_sync BOOLEAN DEFAULT TRUE,
    last_exported_at TIMESTAMPTZ
  );

  Purpose: Staged data for export. Generated from:
  - mentors (source of truth for identity)
  - mn_tasks (for custom field status values)
  - mn_gb_contacts_raw (for contact_id to update existing)

  3. mentors - Our Source of Truth

  Already exists, Jotform-driven.

  Data Flow

  Initial Setup Flow

  1. CSV Upload (first time)
     â”œâ”€ Parse full CSV
     â”œâ”€ INSERT INTO mn_gb_contacts_raw (source='csv_upload')
     â”œâ”€ Match to mentors by phone/email
     â”œâ”€ UPDATE mentors.gb_contact_id WHERE matched
     â””â”€ Detect duplicates â†’ mn_errors

  2. Generate Initial Import
     â”œâ”€ ETL runs â†’ populate mn_gb_import from mentors + mn_tasks
     â””â”€ Left join mn_gb_contacts_raw for existing contact_ids

  Periodic Sync Flow

  1. API Sync (Jotform + GB Members)
     â”œâ”€ Fetch signups, setup, members
     â””â”€ ETL â†’ mentors, mn_tasks

  2. Contact Sync (NEW!)
     FOR EACH mentor WHERE gb_contact_id IS NOT NULL:
       â”œâ”€ GET /contacts/{gb_contact_id}
       â”œâ”€ UPSERT mn_gb_contacts_raw (source='api_sync')
       â””â”€ Compare with mentors table:
           - Detect conflicts â†’ mn_errors
           - Sync custom field changes back

  3. Regenerate mn_gb_import
     â”œâ”€ Clear table
     â”œâ”€ Generate from: mentors + mn_tasks + mn_gb_contacts_raw
     â””â”€ Ready for next export

  Contact Import Prep Flow

  User clicks "Generate Contact Import CSV":
  1. Check freshness of mn_gb_contacts_raw
  2. If stale â†’ warn "Run sync first"
  3. Generate CSV from mn_gb_import table
  4. Download to user

  Initial CSV Upload Logic (Full Parse)

  Since initial upload needs full functionality:

  async function uploadInitialCSV(file: File) {
    const rows = parseCSV(file)

    for (const row of rows) {
      // Parse full contact data
      const contact = {
        contact_id: row['Givebutter Contact ID'],
        first_name: row['First Name'],
        last_name: row['Last Name'],
        prefix: row['Prefix'],
        primary_email: row['Primary Email'],
        primary_phone: row['Primary Phone'],
        tags: row['Tags']?.split(',') || [],
        custom_fields: extractCustomFields(row),
        source: 'csv_upload'
      }

      // Store to raw table
      await db.insert('mn_gb_contacts_raw').values(contact)

      // Try to match to mentor
      const mentor = await findMentorByPhoneOrEmail(
        contact.primary_phone,
        contact.primary_email
      )

      if (mentor) {
        // Update mentor with contact_id
        await db.update('mentors')
          .set({ gb_contact_id: contact.contact_id })
          .where({ mn_id: mentor.mn_id })

        // Link in raw table
        await db.update('mn_gb_contacts_raw')
          .set({ mn_id: mentor.mn_id })
          .where({ contact_id: contact.contact_id })
      }
    }

    // Detect duplicates
    const duplicates = await detectDuplicates()
    // Log to mn_errors
  }

  Contact Sync During Periodic Sync

  async function syncGivebutterContacts() {
    const mentorsWithContactIds = await db
      .select()
      .from('mentors')
      .whereNotNull('gb_contact_id')

    for (const mentor of mentorsWithContactIds) {
      try {
        // Fetch from API
        const gbContact = await fetch(
          `https://api.givebutter.com/v1/contacts/${mentor.gb_contact_id}`
        )

        // Upsert to raw table
        await db.insert('mn_gb_contacts_raw')
          .values({
            contact_id: mentor.gb_contact_id,
            mn_id: mentor.mn_id,
            ...gbContact,
            source: 'api_sync',
            gb_updated_at: gbContact.updated_at
          })
          .onConflict('contact_id')
          .merge()

        // Compare for conflicts
        const conflicts = detectConflicts(mentor, gbContact)
        if (conflicts.length > 0) {
          await logConflicts(mentor.mn_id, conflicts)
        }

        // Sync back custom field updates
        if (gbContact.custom_fields['Preferred Name'] !== mentor.preferred_name) {
          await db.update('mentors')
            .set({ preferred_name: gbContact.custom_fields['Preferred Name'] })
            .where({ mn_id: mentor.mn_id })
        }

      } catch (error) {
        // Log sync error
      }
    }
  }

  Generating mn_gb_import (Always Fresh)

  async function regenerateImportTable() {
    // Clear table
    await db.delete('mn_gb_import').where({})

    const mentors = await db.select().from('mentors')

    for (const mentor of mentors) {
      const task = await getTask(mentor.mn_id)
      const gbContact = await db
        .select()
        .from('mn_gb_contacts_raw')
        .where({ mn_id: mentor.mn_id })
        .first()

      // Generate import row
      const importRow = {
        mn_id: mentor.mn_id,
        'Givebutter Contact ID': gbContact?.contact_id?.toString() || null,
        'Contact External ID': mentor.mn_id,
        'Prefix': mentor.preferred_name,
        'First Name': mentor.first_name,
        'Middle Name': mentor.middle_name,
        'Last Name': mentor.last_name,
        'Primary Email': mentor.personal_email || mentor.uga_email,
        'Primary Phone Number': mentor.phone,
        'Tags': generateTags(mentor.status_category),
        // Custom fields from config + task
        'ðŸ“ Sign Up Complete': task.signup_done ? 'Yes' : 'No',
        'ðŸ’¸ Givebutter Page Setup': task.setup_done ? 'Yes' : 'No',
        // ... etc
        needs_sync: true
      }

      await db.insert('mn_gb_import').values(importRow)
    }
  }

  Amount Raised - Good Catch!

  You're absolutely right:
  campaign_members_raw.amount_raised â†’ mn_tasks.amount_raised

  This is already handled in ETL. The contact API doesn't include fundraising amounts - that comes from the members API, which we're already
  fetching!

  Relational Structure

  mentors (mn_id, gb_contact_id, ...)
     â†“ (1:1)
  mn_gb_contacts_raw (contact_id, mn_id, ...) [what GB currently has]
     â†“ (used to generate)
  mn_gb_import (mn_id, ...) [what we're preparing to send]

  And separately:
  campaign_members_raw (member_id, mn_id, amount_raised)
     â†“ (feeds into)
  mn_tasks (mn_id, amount_raised, ...)
     â†“ (used to generate)
  mn_gb_import (mn_id, custom_fields['$ Raised'])

  Does This Nail It?

  So the key insights:
  1. Initial CSV = full parse, populate mn_gb_contacts_raw, match mentors
  2. Periodic sync = API fetch contacts, upsert to mn_gb_contacts_raw, detect conflicts
  3. mn_gb_contacts_raw = mirror of GB (from CSV or API)
  4. mn_gb_import = always regenerated from mentors + tasks + raw
  5. Amount raised = from members API, not contacts

  This avoids hard-coded merge logic and keeps clear separation. Ready to build?

> Ok yes the only thing left to say is that the mn_gb_contacts_raw should ONLY contain mentor contacts & their gb ids relationally connected to the 
mentor database. The full_gb_export or whatever will store the 40k+ contacts. THhoughts? 

âº Perfect! Yes, this is an important distinction:

  Two Contact Tables

  full_gb_contacts - Complete CSV Dump

  CREATE TABLE full_gb_contacts (
    contact_id INTEGER PRIMARY KEY,
    first_name TEXT,
    last_name TEXT,
    primary_email TEXT,
    primary_phone TEXT,
    tags TEXT[],
    custom_fields JSONB,

    csv_uploaded_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW()
  );

  Purpose:
  - Store ALL 40k+ contacts from Givebutter export
  - Used ONLY for initial matching and duplicate detection
  - Never updated after upload (snapshot in time)
  - Not linked to mentors (too many non-mentor contacts)

  When populated:
  - Initial CSV upload during setup
  - Manual CSV uploads when user needs to refresh matching

  mn_gb_contacts_raw - Mentor Contacts Only

  CREATE TABLE mn_gb_contacts_raw (
    contact_id INTEGER PRIMARY KEY,
    mn_id TEXT UNIQUE REFERENCES mentors(mn_id), -- Always linked!

    -- Full contact data (exact API response)
    first_name TEXT,
    last_name TEXT,
    prefix TEXT,
    primary_email TEXT,
    primary_phone TEXT,
    email_addresses TEXT[],
    phone_numbers TEXT[],
    tags TEXT[],
    custom_fields JSONB,

    source TEXT, -- 'csv_match' | 'api_sync'
    gb_updated_at TIMESTAMPTZ,
    last_synced_at TIMESTAMPTZ DEFAULT NOW(),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
  );

  Purpose:
  - Store ONLY mentor contacts (555 max)
  - Every row MUST have mn_id (relationally connected)
  - Updated from API during periodic sync
  - Source of truth for "what GB currently has for our mentors"

  Data Flow

  Initial CSV Upload

  1. Parse CSV (40k+ contacts)
     â†“
  2. INSERT INTO full_gb_contacts (all contacts)
     â†“
  3. For each contact:
       Try match to mentors (phone/email)
       â†“
       If matched:
         - UPDATE mentors.gb_contact_id
         - INSERT INTO mn_gb_contacts_raw (only matched contacts)
         - UPDATE mn_gb_contacts_raw SET mn_id = mentor.mn_id
     â†“
  4. Result:
     - full_gb_contacts: 40k+ rows (no mn_id)
     - mn_gb_contacts_raw: ~450 rows (all with mn_id)
     - mentors: 555 rows (~450 now have gb_contact_id)

  Periodic API Sync

  1. For each mentor WHERE gb_contact_id IS NOT NULL:
     â†“
  2. GET /contacts/{gb_contact_id}
     â†“
  3. UPSERT INTO mn_gb_contacts_raw
     - Does NOT touch full_gb_contacts
     - Only updates mentor-linked contacts
     â†“
  4. Detect conflicts (compare with mentors table)

  Why Keep full_gb_contacts?

  Use cases:
  1. Re-matching: If mentor info changes, re-run match against full list
  2. Duplicate detection: Find all contacts with same phone/email (might be non-mentor)
  3. Historical reference: "What did GB look like when we last uploaded?"
  4. Future features: Donor tracking, volunteer management, etc.

  Cleanup strategy:
  - Clear before each CSV upload (replace with new snapshot)
  - Or keep timestamped (add upload_id FK to track multiple uploads)

  Table Sizes

  | Table              | Rows     | Purpose              |
  |--------------------|----------|----------------------|
  | full_gb_contacts   | ~40,000  | Complete GB snapshot |
  | mn_gb_contacts_raw | ~555 max | Mentor contacts only |
  | mentors            | ~555     | Our source of truth  |
  | mn_gb_import       | ~555     | Staged export data   |

  Example Queries

  Find mentor's current GB data:
  SELECT * FROM mn_gb_contacts_raw
  WHERE mn_id = '1001';

  Find all GB contacts for a phone (duplicates):
  SELECT * FROM full_gb_contacts
  WHERE primary_phone = '+15551234567';

  Check if mentor has GB contact synced:
  SELECT m.*, gb.last_synced_at
  FROM mentors m
  LEFT JOIN mn_gb_contacts_raw gb ON m.mn_id = gb.mn_id
  WHERE m.mn_id = '1001';

  Updated Flow Diagram

  CSV Upload (40k contacts)
      â†“
  full_gb_contacts (40k, no mn_id)
      â†“ (matching logic)
  mn_gb_contacts_raw (555 max, all with mn_id) â† API sync updates these
      â†“ (linked via mn_id)
  mentors (555, some with gb_contact_id)
      â†“ (generate export)
  mn_gb_import (555, prepared CSV)

  Does This Structure Make Sense?

  - full_gb_contacts = haystack (40k contacts, find our needles)
  - mn_gb_contacts_raw = needles (only mentor contacts, kept fresh via API)
  - Clean separation, no mixing of mentor and non-mentor data

  Ready to lock this in?

> Yes this is all good however, the 2 raw tables (the full export and the raw gb contacts) should both have many more fields than that I think. There
 are feilds like external id which is where we plan to have mentor id, etc. Check the TEST F TEST L contact in the data folder to see all of the 
potential fields we might fill in. This is the only thing I can think of last 

âº Excellent catch! Let me check the actual Givebutter CSV structure:

âº Search(pattern: "**/data/**/*.csv")
  âŽ¿ Â Found 3 files (ctrl+o to expand)

âº Bash(head -2 "/Users/calebsandler/Code Repos/SWAB/mentor-database/backend/data/Full_Export_10.7.csv" | cat)
  âŽ¿ "Givebutter Contact ID","Contact External ID","Prefix","First Name","Middle Name","Last Name","Suffix","Date of Birth","Gender","Employer","Titl
    e","Primary Email","Additional Emails","Primary Phone","Additional Phones","Address Line 1","Address Line 2","City","State","Postal Code","Count
    ry","Additional Addresses","Website","Twitter","LinkedIn","Facebook","Recurring Contributions","Total Contributions","Total Soft Credits","Engag
    â€¦ +7 lines (ctrl+o to expand)




âº Bash(grep -i "TEST F" "/Users/calebsandler/Code Repos/SWAB/mentor-database/backend/data/Full_Export_10.7.csv" | head -1)
  âŽ¿ "15234722","This will be for the mentor ID because I think that is the easiest thing","Timotheetimtamy","TEST F","This is a test for where we co
    uld put the middle name.","TEST L","","","","","Mentor here instead of tags? Not sure","sandlercaleb@gmail.com","cjs06002@uga.edu","+18455687182
    ","","","","","","","","","","","","","$0.00","$0.00","$0.00","TRUE","TRUE","TRUE","Mentors 2024, Internal 2024, Family Fundraising, High Engage
    â€¦ +3 lines (ctrl+o to expand)




âº Perfect! Now I can see the complete structure. Here's the full schema for both tables:

  Complete Table Schemas

  full_gb_contacts - Complete Export (40k+ contacts)

  CREATE TABLE full_gb_contacts (
    -- PRIMARY IDENTIFIERS
    contact_id INTEGER PRIMARY KEY,
    external_id TEXT,  -- Where we store mn_id for OUR contacts

    -- NAME FIELDS
    prefix TEXT,
    first_name TEXT,
    middle_name TEXT,
    last_name TEXT,
    suffix TEXT,

    -- DEMOGRAPHICS
    date_of_birth DATE,
    gender TEXT,
    employer TEXT,
    title TEXT,

    -- CONTACT INFO
    primary_email TEXT,
    additional_emails TEXT,
    primary_phone TEXT,
    additional_phones TEXT,

    -- ADDRESS
    address_line_1 TEXT,
    address_line_2 TEXT,
    city TEXT,
    state TEXT,
    postal_code TEXT,
    country TEXT,
    additional_addresses TEXT,

    -- SOCIAL
    website TEXT,
    twitter TEXT,
    linkedin TEXT,
    facebook TEXT,

    -- GIVEBUTTER METADATA
    recurring_contributions TEXT,
    total_contributions TEXT,
    total_soft_credits TEXT,
    engage_email_subscribed BOOLEAN,
    engage_sms_subscribed BOOLEAN,
    engage_mail_subscribed BOOLEAN,

    -- ORGANIZATION
    tags TEXT[],
    notes TEXT,
    household_id TEXT,
    household TEXT,
    household_primary_contact BOOLEAN,

    -- TIMESTAMPS
    date_created_utc TIMESTAMPTZ,
    last_modified_utc TIMESTAMPTZ,

    -- CUSTOM FIELDS (from custom-fields.json config)
    custom_fields JSONB,  -- Stores all custom fields as JSON

    -- Our metadata
    csv_uploaded_at TIMESTAMPTZ DEFAULT NOW(),
    csv_filename TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
  );

  -- Index for matching
  CREATE INDEX idx_full_gb_contacts_phone ON full_gb_contacts(primary_phone);
  CREATE INDEX idx_full_gb_contacts_email ON full_gb_contacts(primary_email);
  CREATE INDEX idx_full_gb_contacts_external_id ON full_gb_contacts(external_id);

  mn_gb_contacts_raw - Mentor Contacts Only (555 max)

  CREATE TABLE mn_gb_contacts_raw (
    -- PRIMARY IDENTIFIERS
    contact_id INTEGER PRIMARY KEY,
    mn_id TEXT UNIQUE NOT NULL REFERENCES mentors(mn_id), -- Always linked!
    external_id TEXT,  -- Should match mn_id when synced

    -- NAME FIELDS
    prefix TEXT,
    first_name TEXT,
    middle_name TEXT,
    last_name TEXT,
    suffix TEXT,

    -- DEMOGRAPHICS
    date_of_birth DATE,
    gender TEXT,
    employer TEXT,
    title TEXT,

    -- CONTACT INFO
    primary_email TEXT,
    additional_emails TEXT,
    primary_phone TEXT,
    additional_phones TEXT,

    -- ADDRESS
    address_line_1 TEXT,
    address_line_2 TEXT,
    city TEXT,
    state TEXT,
    postal_code TEXT,
    country TEXT,
    additional_addresses TEXT,

    -- SOCIAL
    website TEXT,
    twitter TEXT,
    linkedin TEXT,
    facebook TEXT,

    -- GIVEBUTTER METADATA
    recurring_contributions TEXT,
    total_contributions TEXT,
    total_soft_credits TEXT,
    engage_email_subscribed BOOLEAN,
    engage_sms_subscribed BOOLEAN,
    engage_mail_subscribed BOOLEAN,

    -- ORGANIZATION
    tags TEXT[],
    notes TEXT,
    household_id TEXT,
    household TEXT,
    household_primary_contact BOOLEAN,

    -- TIMESTAMPS
    date_created_utc TIMESTAMPTZ,
    last_modified_utc TIMESTAMPTZ,

    -- CUSTOM FIELDS (dynamically from config)
    custom_fields JSONB,  -- All custom fields including:
                          -- "ðŸ“ Sign Up Complete", "ðŸ’¸ Givebutter Page Setup",
                          -- "ðŸ“† Shift Preference", "ðŸ“±Custom Text Message 1ï¸âƒ£", etc.

    -- SYNC METADATA
    source TEXT CHECK (source IN ('csv_match', 'api_sync')),
    gb_updated_at TIMESTAMPTZ,  -- From API or CSV "Last Modified (UTC)"
    last_synced_at TIMESTAMPTZ DEFAULT NOW(),
    sync_status TEXT CHECK (sync_status IN ('synced', 'conflict', 'stale')),

    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
  );

  -- Indexes
  CREATE INDEX idx_mn_gb_contacts_raw_mn_id ON mn_gb_contacts_raw(mn_id);
  CREATE INDEX idx_mn_gb_contacts_raw_sync_status ON mn_gb_contacts_raw(sync_status);

  Custom Fields Handling

  From the TEST contact, I can see these custom fields:
  - ðŸ“ Sign Up Complete â†’ "Yes"
  - ðŸ’¸ Givebutter Page Setup â†’ "Yes"
  - ðŸ“† Shift Preference â†’ "Option 4"
  - ðŸ‘¯â€â™‚ï¸ Partner Preference â†’ "1,234%"
  - ðŸš‚ Mentor Training Complete â†’ "No"
  - ðŸ“ˆ Fully Fundraised? â†’ "Yes"
  - ðŸ“±Custom Text Message 1ï¸âƒ£ â†’ "Here s the custom text message..."

  Storage strategy:
  -- Store as JSONB
  custom_fields = {
    "ðŸ“ Sign Up Complete": "Yes",
    "ðŸ’¸ Givebutter Page Setup": "Yes",
    "ðŸ“† Shift Preference": "Option 4",
    "ðŸ‘¯â€â™‚ï¸ Partner Preference": "1,234%",
    "ðŸš‚ Mentor Training Complete": "No",
    "ðŸ“ˆ Fully Fundraised?": "Yes",
    "ðŸ“±Custom Text Message 1ï¸âƒ£": "Here s the custom text message..."
  }

  Parsing Logic

  function parseGivebutterCSV(row: any) {
    // Extract custom fields dynamically
    const customFields: Record<string, any> = {};
    const customFieldConfig = loadCustomFieldsConfig();

    customFieldConfig.fields.forEach((field: any) => {
      if (row[field.name]) {
        customFields[field.name] = row[field.name];
      }
    });

    return {
      contact_id: parseInt(row['Givebutter Contact ID']),
      external_id: row['Contact External ID'] || null,
      prefix: row['Prefix'] || null,
      first_name: row['First Name'] || null,
      middle_name: row['Middle Name'] || null,
      last_name: row['Last Name'] || null,
      suffix: row['Suffix'] || null,
      date_of_birth: row['Date of Birth'] ? new Date(row['Date of Birth']) : null,
      gender: row['Gender'] || null,
      employer: row['Employer'] || null,
      title: row['Title'] || null,
      primary_email: row['Primary Email'] || null,
      additional_emails: row['Additional Emails'] || null,
      primary_phone: row['Primary Phone'] || null,
      additional_phones: row['Additional Phones'] || null,
      address_line_1: row['Address Line 1'] || null,
      address_line_2: row['Address Line 2'] || null,
      city: row['City'] || null,
      state: row['State'] || null,
      postal_code: row['Postal Code'] || null,
      country: row['Country'] || null,
      additional_addresses: row['Additional Addresses'] || null,
      website: row['Website'] || null,
      twitter: row['Twitter'] || null,
      linkedin: row['LinkedIn'] || null,
      facebook: row['Facebook'] || null,
      recurring_contributions: row['Recurring Contributions'] || null,
      total_contributions: row['Total Contributions'] || null,
      total_soft_credits: row['Total Soft Credits'] || null,
      engage_email_subscribed: row['Engage Email Subscribed'] === 'TRUE',
      engage_sms_subscribed: row['Engage SMS Subscribed'] === 'TRUE',
      engage_mail_subscribed: row['Engage Mail Subscribed'] === 'TRUE',
      tags: row['Tags'] ? row['Tags'].split(', ') : [],
      notes: row['Notes'] || null,
      household_id: row['Household ID'] || null,
      household: row['Household'] || null,
      household_primary_contact: row['Household Primary Contact'] === 'TRUE',
      date_created_utc: row['Date Created (UTC)'] ? new Date(row['Date Created (UTC)']) : null,
      last_modified_utc: row['Last Modified (UTC)'] ? new Date(row['Last Modified (UTC)']) : null,
      custom_fields: customFields
    };
  }

  Key Points from TEST Contact

  1. External ID = "This will be for the mentor ID..." â†’ We'll set this to mn_id
  2. Prefix = "Timotheetimtamy" â†’ Preferred name
  3. Tags = Multiple tags comma-separated
  4. Custom fields = All our configured fields present
  5. Notes = Free text field for admin comments

  Is This Complete Now?

  This schema captures:
  - âœ… All 58 columns from Givebutter export
  - âœ… Custom fields as JSONB (dynamic based on config)
  - âœ… External ID for mn_id linkage
  - âœ… All metadata fields
  - âœ… Proper indexes for matching

                                                                                                                                                    
âº User approved Claude's plan:
  âŽ¿ Â Create Comprehensive Architecture Documentation

     I will create SYNC_ARCHITECTURE.md in the project root containing:

     Complete Content (comprehensive, not outline):

     1. System Context & Problem Statement
       - Full explanation of Givebutter API limitations
       - Why CSV is necessary despite having an API
       - Members vs Contacts distinction (all members are contacts, not all contacts are members)
       - The feedback loop problem (can't get contact_id from members API)
     2. Complete Database Schema 
       - Every table with ALL fields (not summaries)
       - The full 58-column structure for both GB contact tables
       - Rationale for each table's existence
       - Relationships and foreign keys
       - Custom fields as JSONB strategy
     3. Three-Tier Architecture Deep Dive
       - Full explanation of why three tiers exist
       - Initial Setup: Why CSV is mandatory, complete flow
       - Periodic Sync: Why no CSV, what it does, how often
       - Feature Operations: When needed, what they do
       - Comparison table showing differences
     4. All Five Scenarios (complete step-by-step)
       - Scenario 1: New mentor, first year (full flow)
       - Scenario 2: Returning mentor (matching logic)
       - Scenario 3: CSV feedback loop (export â†’ import â†’ re-upload)
       - Scenario 4: Duplicate contacts (detection and logging)
       - Scenario 5: CSV sync-back issue and API solution
     5. CSV vs API Strategy (the evolution of our thinking)
       - Initial thought: CSV data merge
       - Problem discovered: Two-way sync conflicts
       - Solution: Use API for contact data, CSV only for IDs
       - Why this is cleaner and more reliable
     6. Contact Matching Algorithm
       - Full matching logic with priority order
       - Phone normalization rules
       - Email matching (personal vs UGA)
       - Member ID fallback
       - What happens when multiple matches found
     7. Conflict Detection Logic
       - Field-by-field ownership rules
       - What gets synced back from Givebutter
       - What doesn't (and why)
       - How conflicts are logged
       - Example conflict scenarios
     8. Complete Data Flows
       - Initial setup wizard (multi-step with CSV upload)
       - Periodic sync flow (API + ETL + contact sync)
       - CSV upload for feature operations
       - Export generation process
       - Each with detailed step-by-step breakdown
     9. All Edge Cases (from our discussion)
       - Phone changed, email changed, shared phone, deleted contact, stale CSV, duplicates, partial uploads, no match, multiple matches
       - Each with solution and reasoning
     10. Full Table Schemas (actual SQL)
       - Complete CREATE TABLE statements
       - All 58 GB CSV columns properly mapped
       - Indexes and constraints
       - Why certain design choices were made
     11. API Endpoint Specifications
       - Complete endpoint list with request/response formats
       - When each is used
       - Error handling
     12. UI Organization & User Flows
       - Why each page exists
       - What belongs where (and why)
       - CSV freshness indicators
       - Sync status displays
     13. Implementation Phases
       - What to build in what order (and why)
       - Dependencies between phases
       - What can wait for Phase 2
     14. Key Design Decisions & Rationale
       - Why two contact tables (full vs mentor-only)
       - Why upsert mn_gb_import vs keep persistent
       - Why external_id = mn_id
       - Why custom fields as JSONB
       - Amount raised from members API not contacts
       - Scheduling strategy
     15. Questions Answered & Decisions Made
       - The back-and-forth that led to current design
       - What we considered and rejected (and why)

     This will be a DETAILED, reference-quality document that preserves all the thinking and analysis, not just bullet points. You'll be able to edit
      and verify before any code is written.
  âŽ¿

âº Write(SYNC_ARCHITECTURE.md)
  âŽ¿ Â Wrote 2894 lines to SYNC_ARCHITECTURE.md            
     # Mentor Database Sync Architecture
     **Last Updated:** October 8, 2025
     **Status:** Architecture Design - Pre-Implementation
     ---
     ## Table of Contents
     1. [Executive Summary](#executive-summary)
     â€¦ +2884 lines (ctrl+o to expand)

âº I've created SYNC_ARCHITECTURE.md in the project root with a comprehensive documentation of everything we've discussed.

  The document includes:

  - Complete problem context - Why CSV is necessary, API limitations, the members vs contacts distinction
  - Full database schemas - All tables with every field (including all 58 GB CSV columns)
  - All five scenarios - Step-by-step flows for each use case we identified
  - Three-tier architecture - Initial setup, periodic sync, feature operations
  - Matching algorithm - Complete logic with priority order
  - Conflict detection - Field-by-field ownership rules
  - All edge cases - The 10 edge cases we discussed with solutions
  - Complete API specs - Every endpoint with request/response formats
  - UI organization - What belongs on each page and why
  - Implementation phases - Ordered plan from Phase 1-5
  - Design decisions - The "why" behind every major choice

  This is a ~20,000 word reference document that captures all the reasoning and analysis from our conversation. You can now review, edit, and verify
  the approach before we write any code.



